# rag-studio/backend/app/api/v1/rag_builder.py
"""
RAG Builder API ì—”ë“œí¬ì¸íŠ¸

LangGraph ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚¬ìš©í•œ ì‹œê°ì  RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

from typing import List, Dict, Any, Optional
from datetime import datetime
import uuid
import json

from fastapi import APIRouter, HTTPException, Depends, status, Body
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel, Field

from app.utils.logger import logger
from app.db.session import get_db
from app.core.dependencies import get_current_user
from app.models.user import User

router = APIRouter()


# RAG Builder ìŠ¤í‚¤ë§ˆ ì •ì˜

class NodePosition(BaseModel):
    """ë…¸ë“œ ìœ„ì¹˜"""
    x: float
    y: float


class NodeData(BaseModel):
    """ë…¸ë“œ ë°ì´í„°"""
    label: str
    type: str
    config: Optional[Dict[str, Any]] = {}


class GraphNode(BaseModel):
    """ê·¸ë˜í”„ ë…¸ë“œ"""
    id: str
    type: str  # input, output, process
    position: NodePosition
    data: NodeData


class GraphEdge(BaseModel):
    """ê·¸ë˜í”„ ì—£ì§€"""
    id: str
    source: str
    target: str
    sourceHandle: Optional[str] = None
    targetHandle: Optional[str] = None
    label: Optional[str] = None


class GraphState(BaseModel):
    """ê·¸ë˜í”„ ìƒíƒœ"""
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    viewport: Optional[Dict[str, Any]] = None


class ComponentDefinition(BaseModel):
    """ì»´í¬ë„ŒíŠ¸ ì •ì˜"""
    id: str
    name: str
    category: str
    description: str
    icon: str
    inputs: List[Dict[str, Any]]
    outputs: List[Dict[str, Any]]
    config_schema: Optional[Dict[str, Any]] = None


class PipelineTemplate(BaseModel):
    """íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿"""
    id: str
    name: str
    description: str
    category: str
    graph: GraphState
    created_at: datetime
    updated_at: datetime


# ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ ì •ì˜
AVAILABLE_COMPONENTS = [
    ComponentDefinition(
        id="data_loader",
        name="Data Loader",
        category="Input",
        description="Load documents from various sources",
        icon="ğŸ“„",
        inputs=[],
        outputs=[{"name": "documents", "type": "Document[]"}],
        config_schema={
            "type": "object",
            "properties": {
                "source_type": {
                    "type": "string",
                    "enum": ["file", "url", "database"],
                    "description": "Data source type"
                },
                "path": {
                    "type": "string",
                    "description": "Path or URL to data"
                }
            }
        }
    ),
    ComponentDefinition(
        id="text_splitter",
        name="Text Splitter",
        category="Processing",
        description="Split documents into chunks",
        icon="âœ‚ï¸",
        inputs=[{"name": "documents", "type": "Document[]"}],
        outputs=[{"name": "chunks", "type": "Chunk[]"}],
        config_schema={
            "type": "object",
            "properties": {
                "chunk_size": {
                    "type": "integer",
                    "default": 1000,
                    "description": "Size of each chunk"
                },
                "chunk_overlap": {
                    "type": "integer",
                    "default": 200,
                    "description": "Overlap between chunks"
                },
                "separator": {
                    "type": "string",
                    "default": "\n\n",
                    "description": "Text separator"
                }
            }
        }
    ),
    ComponentDefinition(
        id="embedding_model",
        name="Embedding Model",
        category="Embedding",
        description="Generate embeddings for text chunks",
        icon="ğŸ§ ",
        inputs=[{"name": "chunks", "type": "Chunk[]"}],
        outputs=[{"name": "embeddings", "type": "Embedding[]"}],
        config_schema={
            "type": "object",
            "properties": {
                "model": {
                    "type": "string",
                    "enum": ["openai", "sentence-transformers", "custom"],
                    "default": "openai",
                    "description": "Embedding model to use"
                },
                "model_name": {
                    "type": "string",
                    "default": "text-embedding-3-small",
                    "description": "Specific model name"
                }
            }
        }
    ),
    ComponentDefinition(
        id="vector_store",
        name="Vector Store",
        category="Storage",
        description="Store and retrieve vector embeddings",
        icon="ğŸ“¦",
        inputs=[{"name": "embeddings", "type": "Embedding[]"}],
        outputs=[{"name": "retriever", "type": "Retriever"}],
        config_schema={
            "type": "object",
            "properties": {
                "index_name": {
                    "type": "string",
                    "description": "OpenSearch index name"
                },
                "similarity_metric": {
                    "type": "string",
                    "enum": ["cosine", "euclidean", "dot_product"],
                    "default": "cosine",
                    "description": "Similarity metric"
                }
            }
        }
    ),
    ComponentDefinition(
        id="retriever",
        name="Retriever",
        category="Retrieval",
        description="Retrieve relevant documents",
        icon="ğŸ”",
        inputs=[{"name": "query", "type": "string"}],
        outputs=[{"name": "documents", "type": "Document[]"}],
        config_schema={
            "type": "object",
            "properties": {
                "top_k": {
                    "type": "integer",
                    "default": 5,
                    "description": "Number of documents to retrieve"
                },
                "filter": {
                    "type": "object",
                    "description": "Metadata filters"
                }
            }
        }
    ),
    ComponentDefinition(
        id="llm_chain",
        name="LLM Chain",
        category="Generation",
        description="Generate answers using LLM",
        icon="ğŸ”—",
        inputs=[
            {"name": "query", "type": "string"},
            {"name": "context", "type": "Document[]"}
        ],
        outputs=[{"name": "answer", "type": "string"}],
        config_schema={
            "type": "object",
            "properties": {
                "model": {
                    "type": "string",
                    "default": "gpt-4-turbo-preview",
                    "description": "LLM model to use"
                },
                "temperature": {
                    "type": "number",
                    "default": 0.7,
                    "minimum": 0,
                    "maximum": 2,
                    "description": "Sampling temperature"
                },
                "max_tokens": {
                    "type": "integer",
                    "default": 2000,
                    "description": "Maximum tokens to generate"
                },
                "prompt_template": {
                    "type": "string",
                    "description": "Custom prompt template"
                }
            }
        }
    ),
    ComponentDefinition(
        id="output_parser",
        name="Output Parser",
        category="Output",
        description="Parse and format the output",
        icon="ğŸ’¡",
        inputs=[{"name": "answer", "type": "string"}],
        outputs=[{"name": "formatted_output", "type": "any"}],
        config_schema={
            "type": "object",
            "properties": {
                "format": {
                    "type": "string",
                    "enum": ["text", "json", "markdown"],
                    "default": "text",
                    "description": "Output format"
                }
            }
        }
    )
]


@router.get("/components", response_model=List[ComponentDefinition])
async def get_available_components() -> List[ComponentDefinition]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ RAG ì»´í¬ë„ŒíŠ¸ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        List[ComponentDefinition]: ì»´í¬ë„ŒíŠ¸ ì •ì˜ ëª©ë¡
    """
    return AVAILABLE_COMPONENTS


@router.get("/components/{component_id}", response_model=ComponentDefinition)
async def get_component_details(component_id: str) -> ComponentDefinition:
    """
    íŠ¹ì • ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ
    
    Args:
        component_id: ì»´í¬ë„ŒíŠ¸ ID
        
    Returns:
        ComponentDefinition: ì»´í¬ë„ŒíŠ¸ ì •ì˜
    """
    component = next(
        (c for c in AVAILABLE_COMPONENTS if c.id == component_id),
        None
    )
    
    if not component:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Component not found: {component_id}"
        )
    
    return component


@router.post("/validate", response_model=Dict[str, Any])
async def validate_pipeline_graph(
    graph: GraphState,
    current_user: User = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ ìœ íš¨ì„± ê²€ì¦
    
    Args:
        graph: ê²€ì¦í•  ê·¸ë˜í”„
        current_user: í˜„ì¬ ì‚¬ìš©ì
        
    Returns:
        Dict[str, Any]: ê²€ì¦ ê²°ê³¼
    """
    errors = []
    warnings = []
    
    # ë…¸ë“œ ID ì¤‘ë³µ í™•ì¸
    node_ids = [node.id for node in graph.nodes]
    if len(node_ids) != len(set(node_ids)):
        errors.append("Duplicate node IDs found")
    
    # ì—£ì§€ ìœ íš¨ì„± í™•ì¸
    for edge in graph.edges:
        if edge.source not in node_ids:
            errors.append(f"Edge source '{edge.source}' not found in nodes")
        if edge.target not in node_ids:
            errors.append(f"Edge target '{edge.target}' not found in nodes")
    
    # ì…ë ¥/ì¶œë ¥ ë…¸ë“œ í™•ì¸
    input_nodes = [n for n in graph.nodes if n.type == "input"]
    output_nodes = [n for n in graph.nodes if n.type == "output"]
    
    if not input_nodes:
        warnings.append("No input nodes found")
    if not output_nodes:
        warnings.append("No output nodes found")
    
    # ì—°ê²°ì„± í™•ì¸ (ê°„ë‹¨í•œ ë²„ì „)
    connected_nodes = set()
    for edge in graph.edges:
        connected_nodes.add(edge.source)
        connected_nodes.add(edge.target)
    
    isolated_nodes = set(node_ids) - connected_nodes
    if isolated_nodes:
        warnings.append(f"Isolated nodes found: {list(isolated_nodes)}")
    
    # ê²°ê³¼ êµ¬ì„±
    is_valid = len(errors) == 0
    
    result = {
        "is_valid": is_valid,
        "errors": errors,
        "warnings": warnings,
        "node_count": len(graph.nodes),
        "edge_count": len(graph.edges)
    }
    
    logger.info(f"ê·¸ë˜í”„ ê²€ì¦ ì™„ë£Œ: valid={is_valid}, user={current_user.username}")
    
    return result


@router.post("/compile", response_model=Dict[str, Any])
async def compile_pipeline_graph(
    graph: GraphState,
    pipeline_name: str = Body(...),
    current_user: User = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¡œ ì»´íŒŒì¼
    
    Args:
        graph: ì»´íŒŒì¼í•  ê·¸ë˜í”„
        pipeline_name: íŒŒì´í”„ë¼ì¸ ì´ë¦„
        current_user: í˜„ì¬ ì‚¬ìš©ì
        
    Returns:
        Dict[str, Any]: ì»´íŒŒì¼ ê²°ê³¼
    """
    try:
        # ê·¸ë˜í”„ ê²€ì¦
        validation = await validate_pipeline_graph(graph, current_user)
        if not validation["is_valid"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid graph: {validation['errors']}"
            )
        
        # ë…¸ë“œë¥¼ ì»´í¬ë„ŒíŠ¸ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        nodes_by_type = {}
        for node in graph.nodes:
            node_type = node.data.type
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)
        
        # ì‹¤í–‰ ìˆœì„œ ê²°ì • (ê°„ë‹¨í•œ í† í´ë¡œì§€ ì •ë ¬)
        execution_order = _topological_sort(graph.nodes, graph.edges)
        
        # LangGraph ì½”ë“œ ìƒì„± (ì˜ì‚¬ ì½”ë“œ)
        code_snippet = f"""
# Auto-generated RAG Pipeline: {pipeline_name}
# Generated at: {datetime.utcnow().isoformat()}

from langchain.schema import Document
from langgraph.graph import StateGraph, END

# Define the graph state
class PipelineState(TypedDict):
    query: str
    documents: List[Document]
    embeddings: List[List[float]]
    retrieved_docs: List[Document]
    answer: str

# Create the graph
workflow = StateGraph(PipelineState)

# Add nodes based on components
"""
        
        for node in execution_order:
            component_type = node.data.type
            config = node.data.config or {}
            
            code_snippet += f"""
# Node: {node.data.label} ({component_type})
async def {node.id}_node(state: PipelineState) -> PipelineState:
    # Component configuration: {json.dumps(config, indent=2)}
    # TODO: Implement {component_type} logic
    return state

workflow.add_node("{node.id}", {node.id}_node)
"""
        
        # ì—£ì§€ ì¶”ê°€
        code_snippet += "\n# Add edges\n"
        for edge in graph.edges:
            code_snippet += f'workflow.add_edge("{edge.source}", "{edge.target}")\n'
        
        # íŒŒì´í”„ë¼ì¸ ID ìƒì„±
        pipeline_id = str(uuid.uuid4())
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "pipeline_id": pipeline_id,
            "pipeline_name": pipeline_name,
            "code_snippet": code_snippet,
            "execution_order": [node.id for node in execution_order],
            "component_count": len(graph.nodes),
            "connection_count": len(graph.edges),
            "estimated_latency_ms": len(graph.nodes) * 100,  # ì˜ˆìƒ ì§€ì—°ì‹œê°„
            "compiled_at": datetime.utcnow().isoformat()
        }
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì™„ë£Œ: {pipeline_name}, user={current_user.username}")
        
        return result
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Compilation failed: {str(e)}"
        )


@router.get("/templates", response_model=List[PipelineTemplate])
async def get_pipeline_templates() -> List[PipelineTemplate]:
    """
    ì‚¬ì „ ì •ì˜ëœ íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        List[PipelineTemplate]: í…œí”Œë¦¿ ëª©ë¡
    """
    # ìƒ˜í”Œ í…œí”Œë¦¿ë“¤
    templates = [
        {
            "id": "basic-rag",
            "name": "Basic RAG Pipeline",
            "description": "Simple retrieval-augmented generation pipeline",
            "category": "starter",
            "graph": {
                "nodes": [
                    {
                        "id": "loader",
                        "type": "input",
                        "position": {"x": 100, "y": 100},
                        "data": {
                            "label": "Document Loader",
                            "type": "data_loader",
                            "config": {"source_type": "file"}
                        }
                    },
                    {
                        "id": "splitter",
                        "type": "process",
                        "position": {"x": 300, "y": 100},
                        "data": {
                            "label": "Text Splitter",
                            "type": "text_splitter",
                            "config": {"chunk_size": 1000}
                        }
                    },
                    {
                        "id": "embedder",
                        "type": "process",
                        "position": {"x": 500, "y": 100},
                        "data": {
                            "label": "Embedding Model",
                            "type": "embedding_model",
                            "config": {"model": "openai"}
                        }
                    },
                    {
                        "id": "output",
                        "type": "output",
                        "position": {"x": 700, "y": 100},
                        "data": {
                            "label": "Output",
                            "type": "output_parser",
                            "config": {"format": "text"}
                        }
                    }
                ],
                "edges": [
                    {"id": "e1", "source": "loader", "target": "splitter"},
                    {"id": "e2", "source": "splitter", "target": "embedder"},
                    {"id": "e3", "source": "embedder", "target": "output"}
                ]
            },
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
    ]
    
    return templates


@router.post("/templates/{template_id}/clone", response_model=GraphState)
async def clone_template(
    template_id: str,
    current_user: User = Depends(get_current_user)
) -> GraphState:
    """
    í…œí”Œë¦¿ì„ ë³µì œí•˜ì—¬ ìƒˆ íŒŒì´í”„ë¼ì¸ ìƒì„±
    
    Args:
        template_id: í…œí”Œë¦¿ ID
        current_user: í˜„ì¬ ì‚¬ìš©ì
        
    Returns:
        GraphState: ë³µì œëœ ê·¸ë˜í”„
    """
    # í…œí”Œë¦¿ ì¡°íšŒ (ì‹¤ì œë¡œëŠ” DBì—ì„œ)
    templates = await get_pipeline_templates()
    template = next((t for t in templates if t["id"] == template_id), None)
    
    if not template:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Template not found: {template_id}"
        )
    
    # ìƒˆ IDë¡œ ë…¸ë“œ ë³µì œ
    cloned_graph = template["graph"].copy()
    
    # ë…¸ë“œ ID ì¬ìƒì„±
    id_mapping = {}
    for node in cloned_graph["nodes"]:
        old_id = node["id"]
        new_id = f"{old_id}_{uuid.uuid4().hex[:8]}"
        id_mapping[old_id] = new_id
        node["id"] = new_id
    
    # ì—£ì§€ ID ë° ì°¸ì¡° ì—…ë°ì´íŠ¸
    for edge in cloned_graph["edges"]:
        edge["id"] = f"e_{uuid.uuid4().hex[:8]}"
        edge["source"] = id_mapping.get(edge["source"], edge["source"])
        edge["target"] = id_mapping.get(edge["target"], edge["target"])
    
    logger.info(f"í…œí”Œë¦¿ ë³µì œ ì™„ë£Œ: {template_id}, user={current_user.username}")
    
    return GraphState(**cloned_graph)


def _topological_sort(nodes: List[GraphNode], edges: List[GraphEdge]) -> List[GraphNode]:
    """
    í† í´ë¡œì§€ ì •ë ¬ì„ ì‚¬ìš©í•œ ë…¸ë“œ ì‹¤í–‰ ìˆœì„œ ê²°ì •
    
    Args:
        nodes: ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
        edges: ì—£ì§€ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[GraphNode]: ì •ë ¬ëœ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
    """
    # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    adj_list = {node.id: [] for node in nodes}
    in_degree = {node.id: 0 for node in nodes}
    
    for edge in edges:
        adj_list[edge.source].append(edge.target)
        in_degree[edge.target] += 1
    
    # ì§„ì… ì°¨ìˆ˜ê°€ 0ì¸ ë…¸ë“œë¡œ ì‹œì‘
    queue = [node for node in nodes if in_degree[node.id] == 0]
    result = []
    
    while queue:
        current = queue.pop(0)
        result.append(current)
        
        # ì¸ì ‘ ë…¸ë“œì˜ ì§„ì… ì°¨ìˆ˜ ê°ì†Œ
        for neighbor_id in adj_list[current.id]:
            in_degree[neighbor_id] -= 1
            if in_degree[neighbor_id] == 0:
                neighbor = next(n for n in nodes if n.id == neighbor_id)
                queue.append(neighbor)
    
    # ì‚¬ì´í´ í™•ì¸
    if len(result) != len(nodes):
        raise ValueError("Graph contains a cycle")
    
    return result